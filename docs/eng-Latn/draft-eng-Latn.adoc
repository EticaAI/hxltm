= Multilingual Terminology in Humanitarian Language Exchange: the HXLTM lexicography field guide
EticaAI, Collaborators_of <etica.of.a.ai@gmail.com>; Rocha, Emerson <rocha@ieee.org>
:toc: 1
:toclevels: 4
:version-label: HXLTM Live eddition
:variable-documentation-live-link: https://hxltm.etica.ai/
:variable-python-package-manager-name: hxltm-eticaai
:variable-python-package-manager-released-version: 0.8.9rc2

////
- https://docs.asciidoctor.org/asciidoc/latest/syntax-quick-reference/
////

<<<

== Preface

This book, both available online on {variable-documentation-live-link} and as an ebook,
is designed as a field guide for person's doing practical lexicography and need a kickstart on how to use HXLTM tools.


[NOTE]
.Non-audience (for this field guide)
====
If you are either an academic researcher,
technology expert or interested in help with translations,
please contact the authors directly.
====

////
It is still recommended to take a quick look at this guide.
The public domain license is more than flexible.
This means you can
////

<<<

== Getting started

It's essential that you have knowledge of Arabic numerals and the Latin alphabet without the aid of computer assisted tools.
Attention to details is for such numbers and the Latin alphabet (since they often are used as part of coding systems to label lingístic content)
is much more important than knowledge of the language written in Latin script or skill to do computation with numbers themselves.

=== HXLTM reference tooling installation

Some knowledge of how to operate command line tools is helpful.
No programming skills are required.

All functionality intended for field lexicographers using HXLTM documented here either exposed as _declarative programming_ with YAML or arguments for programs should be considered a bug.

Choose at least one of the options.

==== Python package alternative

hxltm-eticaai Python package:: https://pypi.org/project/hxltm-eticaai/
Pypi, Python package manager:: https://pip.pypa.io/
Python:: https://www.python.org/

===== Active use

[subs="attributes"]
[source,bash]
----
pip install {variable-python-package-manager-name} --upgrade
----


===== Conservative

[subs="attributes"]
[source,bash]
----
pip install {variable-python-package-manager-name}={variable-python-package-manager-released-version}
----

==== Docker alternative

#TODO: this is a draft. Will eventually be updated (2021-11-16T23:16:00Z)#

==== GitHub Action alternative

#TODO: this is a draft. Will eventually be updated (2021-11-16T23:16:00Z)#

=== Production usage

The HXLTM reference tooling is designed to be immediately production grade.
However, if software used humanitarian usage is not challenging enough,
most softwares which advertise conformance with formats HXLTM can export/import actually have bugs.

How to meet both usages?
What "backward compatibility" could we promess?

Here are some suggestions.

==== Active
If your use case is processing data right now, or what you care about is just the end result, you can use the latest versions of any tool.

Even if between the day you start using the tools and a new update some minor change,
it's more likely that you prefer this cutting edge approach.

==== Conservative

In special data standards who already are not formally documented (or their documentation may be behind paywalls) they are subject to changes (which tend to be bug fixes).
The way on how to label these standards may also may also renamed over time.

One common industry practice would be to lock the exact command line tools version (so this would force the old ontologia file). This is still an option.
But HXLTM allows another strategy:
*a conservative approach is to get the relevant parts of the ontologia with a custom name*.
This way you have your own "custom" data standard and it's even documented on your project.

This approach doesn't need to happen upfront. It could be done if something changes and you prefer the old way.

If you think the new changes do not conform to documented standards,
please report it as a bug,
even if it allows you to revert old behavior without ask changes on code.

==== Archival
The "archival" behavior is likely to be the one used for either academic researchers,
librarians or higher levels of compliance (like use HXLTM to explain some custom format).
The difference in this approach is when just freezing the exact command line tools version is not sufficient.
Your use case cannot be sure if you need the exact versions for more than 3 years or 30 years, maybe much more.

The HXLTM reference tools are public domain.
While at time of code writing most PEP8 code conventions were followed,
the maximum lines per file was not,
so less, much less files and no bureaucracy.
Even the characters per line are capped at 80 columns,
so it would fit on paper.
These types of decisions are simpler if you want to archive everything as part of your work.

While this strategy suggestion may resemble how software like Apollo 11 was reconstructed via code printed on books,
note that the average lexicographer would somewhat work with old dictionaries.
Most content from public dictionaries we have today we're extracted from century-old books.

This approach makes the way HXLTM is explained on the ontologia an acceptable trade off between adding new features and allowing such level of archival.

[#HXLTM-TLDR]
=== TL;DR: Too Long; Didn't read

==== hxltmcli

Use case: _"I need convert from HXLTM to something else"_

[source,txt]
----
include::../bin/hxltmcli.py[tag=epilogum]
----

==== hxltmdexml

Use case: _"I need convert from something else (in XML) to HXLTM"_

[source,txt]
----
include::../bin/hxltmdexml.py[tag=epilogum]
----

<<<

== Introduction
:sectnums:

=== How to interpret this book

==== A self-testing approach

Parts of this field guide show, in this order:

* relevant section of the ontologia, _as it is_,
* example of command line usage to trigger data conversion
* Example of one or more generated outputs

At time of writing, the software used is "asciidoctor",
which is focused on technical documentation
This is more general purpose than usage of alternatives like "Jupiter Notebook" popular on data science projects.
Both cases also mix documentation, formulas and outputs.

One disadvantage compared to a more manual approach,
(think any average technical book),
is the end visual result will tend to be more verbose and take more screen size.
Part of this is also the very nature of what HXLTM tools do:
convert between data conventions and everything output is different and every part,
Including new lines or extra spaces,
can be relevant for who is debugging.

One of the reasons for such an approach is greater assurance that the documentation will be updated since several parts of this are literally generated with each release.
New data conventions can be added (or removed) over time, but this approach focuses on making it easier to release more frequently.

This book obviously do not contain all tests that are done, but what is here,
feature-by-feature (or bug-by-bug) what is used on the field.

////
(...)

Sadly, data formats which are not importable back (so a export+import could be used to test)
and do not exist some automated strategy to check of they are valid are extra hard to make full automated checks.
////

<<<

[#HXLTM-archivum-dialecton]
== HXLM core file dialects
:sectnums:

[#HXLTM]
=== `+HXLTM+`
==== Terminologia Multilinguae (priore HXL Trānslātiōnem Memoriam), Datum ideam

////
Naming the HXL TM

Terminologia Multilinguae

- https://en.wiktionary.org/wiki/terminus#Latin
- https://en.wiktionary.org/wiki/logium#Latin
- https://en.wiktionary.org/wiki/multi-#English
- https://en.wiktionary.org/wiki/lingua#Latin
- https://en.wiktionary.org/wiki/multilingual
lingua
////



[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=normam_HXLTM]
----

===== Command line examples

[source,bash]
----
include::../testum/disciplinam-manuale-anglicum.sh[tag=HXLTM_CSV]
----

////
[#CSV-HXL-XLIFF]
=== `+CSV-HXL-XLIFF+`:
==== HXLated bilingual CSV (+ up to 5 source alt) for XLIFF

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=normam_CSV-HXL-XLIFF]
----

===== Command line examples

[source,bash]
----
include::../testum/disciplinam-manuale-anglicum.sh[tag=CSV-HXL-XLIFF]
----
////

[#HXLTM-TMETA]
=== `+HXLTM-TMETA+`:
==== HXLTM Terminologia Multilinguae Meta

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=normam_HXLTM-TMETA]
----

////
===== Command line examples

[source,bash]
----
include::../testum/disciplinam-manuale-anglicum.sh[tag=HXLTM-ASA]
----
////

[#HXLTM-ASA]
=== `+HXLTM-ASA+`:
==== HXLTM Abstractum Syntaxim Arborem

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=normam_HXLTM-ASA]
----


===== Command line examples

[source,bash]
----
include::../testum/disciplinam-manuale-anglicum.sh[tag=HXLTM-ASA]
----

[#HXLTM-archivum-exportandum]
== HXLTM Normam (HXLTM interoperability with conventions/standards)

TIP: The `hxltmcli` and (for importing XML, as long as you map the
     tags and attributes, as this page already do for TMX, TBX, XLIFFs, ...)
     `+hxltmdexml+` *are designed to work with gigabyte size datasets*.
     The ontology file can be customized with `--archivum-configurationem`
     which means both edit or create new exporters/importers are possible.

////
TIP: Consider the formats here as example of how to export HXLTM.
     `hxltmcli --archivum-configurationem path/to/mycopy.hxltm.yml`.
////


[#CSV-3]
=== `+CSV-3+`:
==== CSV 3 bilingual Source + Objective + Comment

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=normam_CSV-3]
----

===== Command line examples


[source,bash]
----
include::../testum/disciplinam-manuale-anglicum.sh[tag=CSV-3]
----

===== Result example

- https://github.com/EticaAI/hxltm/blob/main/testum/resultatum/hxltm-exemplum-linguam.por-Latn_spa-Latn.csv

[source,csv]
----
include::../testum/resultatum/hxltm-exemplum-linguam.por-Latn_spa-Latn.csv[]
----

[#GSheets]
=== `+GSheets+`:
==== Google Sheets, HXLTM container (read-only; native support as data source)

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=normam_GSheets]
----

[#HXL-Proxy]
=== `+HXL-Proxy+`:
==== HXL-Proxy (read-only; native support as data source)

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=normam_HXL-Proxy]
----

[#JSON-kv]
=== `+JSON-kv+`:
==== JSON key: val; id/source -> target (draft)

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=normam_JSON-kv]
----

[#TBX-Basim]
=== `+TBX-Basim+`:
==== TermBase eXchange (TBX) Basic 2.1
////
==== TermBase eXchange (TBX) Basic 2.1
////

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=normam_TBX-Basim]
----

===== Command line examples

[source,bash]
----
include::../testum/disciplinam-manuale-anglicum.sh[tag=TBX-Basim]
----

===== Result example

[source,xml]
----
include::../testum/resultatum/hxltm-exemplum-linguam.tbx[]
----


[#TSV-3]
=== `+TSV-3+`:
==== TSV-3 bilingual Source + Objective + Comment

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=normam_TSV-3]
----

===== Command line examples


[source,bash]
----
include::../testum/disciplinam-manuale-anglicum.sh[tag=TSV-3]
----

===== Result example

- https://github.com/EticaAI/hxltm/blob/main/testum/resultatum/hxltm-exemplum-linguam.por-Latn_spa-Latn.tsv

[source,tsv]
----
include::../testum/resultatum/hxltm-exemplum-linguam.por-Latn_spa-Latn.tsv[]
----

[#TMX]
=== `+TMX+`:
==== Translation Memory eXchange format (TMX)

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=normam_TMX]
----

===== Command line examples

[source,bash]
----
include::../testum/disciplinam-manuale-anglicum.sh[tag=TMX]
----
===== Result example

- https://github.com/EticaAI/hxltm/blob/main/testum/resultatum/hxltm-exemplum-linguam.tmx

[source,xml]
----
include::../testum/resultatum/hxltm-exemplum-linguam.tmx[]
----

[#UTX]
=== `+UTX+`:
==== Universal Terminology eXchange (UTX) (working draft)

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=normam_UTX]
----

===== Command line examples

[source,bash]
----
include::../testum/disciplinam-manuale-anglicum.sh[tag=UTX]
----
===== Result example

- https://github.com/EticaAI/hxltm/blob/main/testum/resultatum/hxltm-exemplum-linguam.utx

[source,xml]
----
include::../testum/resultatum/hxltm-exemplum-linguam.utx[]
----

[#XML]
=== `+XML+`:
==== XML Glōssārium, HXLTM container (generic multilingual XML)'

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=normam_XML]
----

===== Command line examples

[source,bash]
----
include::../testum/disciplinam-manuale-anglicum.sh[tag=XML]
----
===== Result example

- https://github.com/EticaAI/hxltm/blob/main/testum/resultatum/hxltm-exemplum-linguam.tmx

[source,xml]
----
include::../testum/resultatum/hxltm-exemplum-linguam.hxltm.xml[]
----


[#XLIFF]
=== `+XLIFF+`:
==== XML Localization Interchange File Format (XLIFF) v2.1

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=normam_XLIFF]
----

===== Command line examples

[source,bash]
----
include::../testum/disciplinam-manuale-anglicum.sh[tag=XLIFF]
----

===== Result example

[source,xml]
----
include::../testum/resultatum/hxltm-exemplum-linguam.por-Latn--spa-Latn.xlf[]
----

[#XLIFF-obsoletum]
=== `+XLIFF-obsoletum+`:
==== XML Localization Interchange File Format (XLIFF) v1.2

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=normam_XLIFF-obsoletum]
----

===== Command line examples

[source,bash]
----
include::../testum/disciplinam-manuale-anglicum.sh[tag=XLIFF-obsoletum]
----

===== Result example

[source,xml]
----
include::../testum/resultatum/hxltm-exemplum-linguam.por-Latn--spa-Latn.xlf[]
----

[#XLSX]
=== `+XLSX+`:
==== Microsoft Excel, HXLTM container (read-only; native support as data source)

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=normam_XLSX]
----

[#YAML]
=== `+YAML+`:
==== YAML (planned, but no draft)

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=normam_YAML]
----

[#HXLTM-ad-hoc]
== HXLTM Ad Hoc Fōrmulam (HXLTM templated export)

TIP: *About create new HXLTM Ad Hoc*: different from customizable
     link:++#HXLTM-archivum-exportandum++[HXLTM Normam]
     (support for gigabyte size data manipulation) the use of this strategy
     is more optimized for end user who is unlikely o care about load data
     in chunks and try to explain how to import back to HXLTM working file.

NOTE: TODO: this is a draft. Document already implemented functionality

////
= Appendix
:appendix-caption: Exhibit
:sectnums:
:toc:
////

== HXLTM ontologies

[#ontologia]
=== Ontologia

Full file at https://github.com/EticaAI/hxltm/blob/main/ontologia/cor.hxltm.215.yml.

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=ontologia]
----

[#ontologia-aliud]
=== Aliases

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=ontologia_aliud]
----

[#ontologia-datum-typum]
=== Data types

[source,yaml]
----
include::../ontologia/cor.hxltm.215.yml[tag=ontologia_datum_typum]
----

[#hxltmcli--help]
== `+hxltmcli --help+`

[source,bash]
----
# hxltmcli can be installed with hxltm-eticaai
# @see https://pypi.org/project/hxltm-eticaai/
pip install hxltm-eticaai

hxltmcli --help
----

[source]
----
include::../testum/hxltmcli--help_eng-Latn.txt[]
----
[#hxltmdexml--help]
== `+hxltmdexml --help+`

[source,bash]
----
# hxltmdexml can be installed with hxltm-eticaai
# @see https://pypi.org/project/hxltm-eticaai/
pip install hxltm-eticaai

hxltmdexml --help
----

[source]
----
include::../testum/hxltmdexml--help_eng-Latn.txt[]
----

== Glossary
:sectnums:

=== Arabic numerals

#TODO: this is a draft. Will eventually be updated (2021-11-16T23:45:00Z)#

== License

link:UNLICENSE[image:../img/public-domain.png[Public Domain Dedication]]

The https://github.com/EticaAI[EticaAI] has dedicated the work to the
link:../../UNLICENSE[public domain] by waiving all of their rights to the
work worldwide under copyright law, including all related and
neighboring rights, to the extent allowed by law. You can copy, modify,
distribute and perform the work, even for commercial purposes, all
without asking permission.
