= Multilingual Terminology in Humanitarian Language Exchange: the HXLTM lexicography field guide
:toc: 1

////
- https://docs.asciidoctor.org/asciidoc/latest/syntax-quick-reference/
////

<<<

== Preface

This book, both available online and as an ebook, is designed as a field guide for person's doing practical lexicography and need a kickstart on how to use HXLTM tools.

=== Non-audience
If you are either an academic researcher, technology expert or interested in help with translations, please contact the authors directly.


////
It is still recommended to take a quick look at this guide.
The public domain license is more than flexible.
This means you can
////


== Getting started

It's essential that you have knowledge of Arabic numerals and the Latin alphabet without the aid of computer assisted tools.
Attention to details is for such numbers and the Latin alphabet (since they often are used as part of coding systems to label lingÃ­stic content)
is much more important than knowledge of the language written in Latin script or skill to do computation with numbers themselves.

=== HXLTM reference tooling installation

Some knowledge of how to operate command line tools is helpful.
No programming skills are required.

All functionality intended for field lexicographers using HXLTM documented here either exposed as _declarative programming_ with YAML or arguments for programs should be considered a bug.

Choose at least one of the options.

==== Local installation

[source,bash]
----
pip install hxltm-eticaai
----


==== Docker

#TODO: this is a draft. Will eventually be updated (2021-11-16T23:16:00Z)#

==== GitHub Action

#TODO: this is a draft. Will eventually be updated (2021-11-16T23:16:00Z)#

=== Production usage

The HXLTM reference tooling is designed to be immediately production grade.
However, if software used humanitarian usage is not challenging enough,
most softwares which advertise conformance with formats HXLTM can export/import actually have bugs.

How to meet both usages?
What "backward compatibility" could we promess?

Here are some suggestions.

==== Active
If your use case is processing data right now, or what you care about is just the end result, you can use the latest versions of any tool.

Even if between the day you start using the tools and a new update some minor change,
it's more likely that you prefer this cutting edge approach.

==== Conservative

In special data standards who already are not formally documented (or their documentation may be behind paywalls) they are subject to changes (which tend to be bug fixes).
The way on how to label these standards may also may also renamed over time.

One common industry practice would be to lock the exact command line tools version (so this would force the old ontologia file). This is still an option.
But HXLTM allows another strategy:
*a conservative approach is to get the relevant parts of the ontologia with a custom name*.
This way you have your own "custom" data standard and it's even documented on your project.

This approach doesn't need to happen upfront. It could be done if something changes and you prefer the old way.

If you think the new changes do not conform to documented standards,
please report it as a bug,
even if it allows you to revert old behavior without ask changes on code.

==== Archival
The "archival" behavior is likely to be the one used for either academic researchers,
librarians or higher levels of compliance (like use HXLTM to explain some custom format).
The difference in this approach is when just freezing the exact command line tools version is not sufficient.
Your use case cannot be sure if you need the exact versions for more than 3 years or 30 years, maybe much more.

The HXLTM reference tools are public domain.
While at time of code writing most PEP8 code conventions were followed,
the maximum lines per file was not,
so less, much less files and no bureaucracy.
Even the characters per line are capped at 80 columns,
so it would fit on paper.
These types of decisions are simpler if you want to archive everything as part of your work.

While this strategy suggestion may resemble how software like Apollo 11 was reconstructed via code printed on books,
note that the average lexicographer would somewhat work with old dictionaries.
Most content from public dictionaries we have today we're extracted from century-old books.

This approach makes the way HXLTM is explained on the ontologia an acceptable trade off between adding new features and allowing such level of archival.

[#HXLTM-TLDR]
=== TL;DR: Too Long; Didn't read

==== hxltmcli

Use case: _"I need convert from HXLTM to something else"_

[source,txt]
----
include::../bin/hxltmcli.py[tag=epilogum]
----

==== hxltmdexml

Use case: _"I need convert from something else (in XML) to HXLTM"_

[source,txt]
----
include::../bin/hxltmdexml.py[tag=epilogum]
----

<<<

== How to interpret this book

=== A self-testing approach

Parts of this field guide show, in this order:

* relevant section of the ontologia, _as it is_,
* example of command line usage to trigger data conversion
* Example of one or more generated outputs

At time of writing, the software used is "asciidoctor",
which is focused on technical documentation
This is more general purpose than usage of alternatives like "Jupiter Notebook" popular on data science projects.
Both cases also mix documentation, formulas and outputs.

One disadvantage compared to a more manual approach,
(think any average technical book),
is the end visual result will tend to be more verbose and take more screen size.
Part of this is also the very nature of what HXLTM tools do:
convert between data conventions and everything output is different and every part,
Including new lines or extra spaces,
can be relevant for who is debugging.

One of the reasons for such an approach is greater assurance that the documentation will be updated since several parts of this are literally generated with each release.
New data conventions can be added (or removed) over time, but this approach focuses on making it easier to release more frequently.

This book obviously do not contain all tests that are done, but what is here, feature-by-feature (or bug-by-bug) 


(...)


Sadly, data formats which are not importable back (so a export+import could be used to test)
and do not exist some automated strategy to check of they are valid are extra hard to make full automated checks.



